{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059b38bc",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_curve, auc, classification_report\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03ffb4",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        return F_loss\n",
    "\n",
    "class CarotidDataset(Dataset):\n",
    "    def __init__(self, us_images_dir, mask_images_dir, transform=None):\n",
    "        self.us_images = sorted([os.path.join(us_images_dir, fname) for fname in os.listdir(us_images_dir)])\n",
    "        self.mask_images = sorted([os.path.join(mask_images_dir, fname) for fname in os.listdir(mask_images_dir)])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.us_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        us_image = Image.open(self.us_images[idx]).convert('L')\n",
    "        mask = Image.open(self.mask_images[idx]).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            us_image = self.transform(us_image)\n",
    "            mask = self.transform(mask)\n",
    "        return us_image, mask\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                                 stride=stride, padding=padding, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.depthwise(x))\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Conv2d(in_channels, 2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.decoder = nn.ConvTranspose2d(2, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encoder(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = 32\n",
    "        self.patch_embed = nn.Conv2d(in_channels, self.embed_dim, \n",
    "                                    kernel_size=patch_size, stride=patch_size)\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim, \n",
    "            nhead=2,\n",
    "            dim_feedforward=64\n",
    "        )\n",
    "        self.reconstruct = nn.ConvTranspose2d(\n",
    "            self.embed_dim, out_channels,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.reconstruct(x)\n",
    "        return x\n",
    "\n",
    "class Patches(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H % self.patch_size == 0 and W % self.patch_size == 0, \"Image dimensions must be divisible by patch size\"\n",
    "        \n",
    "        num_patches_h = H // self.patch_size\n",
    "        num_patches_w = W // self.patch_size\n",
    "        num_patches = num_patches_h * num_patches_w\n",
    "        \n",
    "        x = x.unfold(2, self.patch_size, self.patch_size)\n",
    "        x = x.unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(B, C, num_patches, self.patch_size * self.patch_size)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.view(B, num_patches, -1)\n",
    "        return x\n",
    "\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, num_patches, projection_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Linear(patch_size * patch_size * 1, projection_dim)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, projection_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x + self.position_embedding\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, projection_dim, num_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(projection_dim)\n",
    "        self.attn = nn.MultiheadAttention(projection_dim, num_heads, dropout=0.1)\n",
    "        self.norm2 = nn.LayerNorm(projection_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(projection_dim, projection_dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(projection_dim * mlp_ratio, projection_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class FusionViTNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, image_size=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = 16\n",
    "        self.projection_dim = 64\n",
    "        self.num_heads = 4\n",
    "        self.transformer_layers = 8\n",
    "        \n",
    "        assert image_size % self.patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        self.num_patches = (image_size // self.patch_size) ** 2\n",
    "        \n",
    "        # CNN Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            DepthwiseSeparableConv(16, 32, stride=2),\n",
    "            nn.ReLU(),\n",
    "            DepthwiseSeparableConv(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ViT components\n",
    "        self.patches = Patches(self.patch_size)\n",
    "        self.patch_encoder = PatchEncoder(\n",
    "            num_patches=self.num_patches,\n",
    "            projection_dim=self.projection_dim,\n",
    "            patch_size=self.patch_size\n",
    "        )\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(self.projection_dim, self.num_heads) \n",
    "            for _ in range(self.transformer_layers)\n",
    "        ])\n",
    "        self.vit_norm = nn.LayerNorm(self.projection_dim)\n",
    "        \n",
    "        # Fusion components\n",
    "        self.fusion_conv = nn.Conv2d(32 + self.projection_dim, 32, kernel_size=1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.Conv2d(16, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == W == 256, f\"Input size must be 256x256, got {H}x{W}\"\n",
    "        \n",
    "        # CNN pathway\n",
    "        cnn_features = self.encoder(x)\n",
    "        \n",
    "        # ViT pathway\n",
    "        patches = self.patches(x)\n",
    "        encoded_patches = self.patch_encoder(patches)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            encoded_patches = block(encoded_patches)\n",
    "        vit_features = self.vit_norm(encoded_patches)\n",
    "        \n",
    "        h = w = int(math.sqrt(self.num_patches))\n",
    "        vit_features = vit_features.transpose(1, 2).view(B, self.projection_dim, h, w)\n",
    "        vit_features = F.interpolate(vit_features, size=cnn_features.shape[2:], mode='bilinear')\n",
    "        \n",
    "        fused_features = torch.cat([cnn_features, vit_features], dim=1)\n",
    "        fused_features = self.fusion_conv(fused_features)\n",
    "        \n",
    "        out = self.decoder(fused_features)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab3c38",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd704656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_run_folder(base_dir='checkpoints'):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    existing_runs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.startswith('run')]\n",
    "    run_numbers = [int(d.replace('run', '')) for d in existing_runs if d.replace('run', '').isdigit()]\n",
    "    next_run_number = max(run_numbers) + 1 if run_numbers else 1\n",
    "    run_folder = os.path.join(base_dir, f'run{next_run_number}')\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    return run_folder\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, val_loss, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "    }, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8493e73",
   "metadata": {},
   "source": [
    "Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            if outputs.shape[2:] != targets.shape[2:]:\n",
    "                outputs = F.interpolate(outputs, size=targets.shape[2:], mode='bilinear')\n",
    "            \n",
    "            # Convert to probabilities and threshold at 0.5\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "            \n",
    "            # Binarize the targets if they're not already binary\n",
    "            if np.max(targets) > 1 or np.min(targets) < 0:\n",
    "                targets = (targets > 0.5).astype(np.float32)\n",
    "            \n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(targets)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Flatten the arrays for metric calculation\n",
    "    preds_flat = all_preds.flatten()\n",
    "    targets_flat = all_targets.flatten()\n",
    "    \n",
    "    # Binarize predictions for metrics that need binary inputs\n",
    "    binary_preds = (preds_flat > 0.5).astype(np.float32)\n",
    "    binary_targets = (targets_flat > 0.5).astype(np.float32)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    tp = np.sum((binary_preds == 1) & (binary_targets == 1))\n",
    "    tn = np.sum((binary_preds == 0) & (binary_targets == 0))\n",
    "    fp = np.sum((binary_preds == 1) & (binary_targets == 0))\n",
    "    fn = np.sum((binary_preds == 0) & (binary_targets == 1))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'IoU': tp / (tp + fp + fn + epsilon),\n",
    "        'Dice': 2 * tp / (2 * tp + fp + fn + epsilon),\n",
    "        'Precision': tp / (tp + fp + epsilon),\n",
    "        'Sensitivity': tp / (tp + fn + epsilon),\n",
    "        'Specificity': tn / (tn + fp + epsilon),\n",
    "        'MSE': np.mean((preds_flat - targets_flat) ** 2),\n",
    "    }\n",
    "    \n",
    "    # Only calculate mAP if we have proper binary targets\n",
    "    if len(np.unique(binary_targets)) >= 2:  # Need both classes present\n",
    "        metrics['mAP'] = average_precision_score(binary_targets, preds_flat)\n",
    "    else:\n",
    "        metrics['mAP'] = float('nan')\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a45e63",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acfb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(model, epoch, train_losses, val_losses, run_folder):\n",
    "    plt.plot(range(1, epoch + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epoch + 1), val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model.__class__.__name__}')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(run_folder, 'training_loss.png'))\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd32698",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "batch_size = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Add this\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Initialize datasets and loaders\n",
    "train_dataset = CarotidDataset(\n",
    "    us_images_dir='Common Carotid Artery Ultrasound Images/US images/train',\n",
    "    mask_images_dir='Common Carotid Artery Ultrasound Images/Expert mask images/train',\n",
    "    transform=transform,\n",
    ")\n",
    "val_dataset = CarotidDataset(\n",
    "    us_images_dir='Common Carotid Artery Ultrasound Images/US images/val',\n",
    "    mask_images_dir='Common Carotid Artery Ultrasound Images/Expert mask images/val',\n",
    "    transform=transform,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "learning_rate_simpleunet = 1e-3\n",
    "learning_rate_vit = 1e-4\n",
    "learning_rate_fusionvitnet = 1e-4\n",
    "criterion = FocalLoss(alpha=0.8, gamma=2.0)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, train_loader, val_loader, num_epochs=epochs, run_folder=None):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    model_folder = os.path.join(run_folder, model.__class__.__name__.lower())\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    \n",
    "    # Create CSV file to store epoch metrics\n",
    "    metrics_file = os.path.join(model_folder, 'epoch_metrics.csv')\n",
    "    metrics_header = ['Epoch', 'Train_Loss', 'Val_Loss', 'IoU', 'Dice', 'Precision', \n",
    "                     'Sensitivity', 'Specificity', 'MSE', 'mAP']\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        f.write(','.join(metrics_header) + '\\n')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, masks = images.to(device), masks.to(device).float()\n",
    "            if masks.ndim == 4 and masks.shape[1] == 3:\n",
    "                masks = masks.mean(dim=1, keepdim=True)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                outputs_resized = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear')\n",
    "                masks_resized = F.interpolate(masks, size=outputs_resized.shape[2:], mode='nearest')\n",
    "                loss = criterion(outputs_resized, masks_resized)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate validation loss and metrics\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device).float()\n",
    "                if masks.ndim == 4 and masks.shape[1] == 3:\n",
    "                    masks = masks.mean(dim=1, keepdim=True)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    outputs_resized = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear')\n",
    "                    masks_resized = F.interpolate(masks, size=outputs_resized.shape[2:], mode='nearest')\n",
    "                    val_loss += criterion(outputs_resized, masks_resized).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate metrics for this epoch\n",
    "        epoch_metrics = calculate_metrics(model, val_loader, device)\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        metrics_row = [\n",
    "            epoch + 1,\n",
    "            train_loss,\n",
    "            val_loss,\n",
    "            epoch_metrics['IoU'],\n",
    "            epoch_metrics['Dice'],\n",
    "            epoch_metrics['Precision'],\n",
    "            epoch_metrics['Sensitivity'],\n",
    "            epoch_metrics['Specificity'],\n",
    "            epoch_metrics['MSE'],\n",
    "            epoch_metrics['mAP']\n",
    "        ]\n",
    "        with open(metrics_file, 'a') as f:\n",
    "            f.write(','.join(map(str, metrics_row)) + '\\n')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, optimizer, epoch, best_val_loss,\n",
    "                          os.path.join(model_folder, f'{model.__class__.__name__}_best.pth'))\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Metrics: {epoch_metrics}\")\n",
    "        plot_training_results(model, epoch + 1, train_losses, val_losses, model_folder)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8bf0b",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    run_folder = create_run_folder()\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'SimpleUNet': SimpleUNet().to(device),\n",
    "        'SimpleViT': SimpleViT().to(device),\n",
    "        'FusionViTNet': FusionViTNet().to(device)\n",
    "    }\n",
    "    \n",
    "    optimizers = {\n",
    "        'SimpleUNet': optim.Adam(models['SimpleUNet'].parameters(), lr=learning_rate_simpleunet),\n",
    "        'SimpleViT': optim.Adam(models['SimpleViT'].parameters(), lr=learning_rate_vit),\n",
    "        'FusionViTNet': optim.Adam(models['FusionViTNet'].parameters(), \n",
    "                                 lr=learning_rate_fusionvitnet, weight_decay=1e-5)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} Model...\")\n",
    "        train_losses, val_losses = train_model(\n",
    "            model, optimizers[name], train_loader, val_loader, epochs, run_folder)\n",
    "        \n",
    "        metrics = calculate_metrics(model, val_loader, device)\n",
    "        results[name] = metrics\n",
    "        print(f\"{name} Metrics: {metrics}\")\n",
    "    \n",
    "    # Save final results\n",
    "    df = pd.DataFrame(results).T.round(4)\n",
    "    df.to_csv(os.path.join(run_folder, 'final_metrics.csv'))\n",
    "    \n",
    "    print(\"\\nTraining and evaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
